{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename, byteorder='>'):\n",
    "    \n",
    "    #first we read the image, as a raw file to the buffer\n",
    "    with open(filename, 'rb') as f:\n",
    "        buffer = f.read()\n",
    "    \n",
    "    #using regex, we extract the header, width, height and maxval of the image\n",
    "    header, width, height, maxval = re.search(\n",
    "        b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n",
    "    \n",
    "    #then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array\n",
    "    return np.frombuffer(buffer,\n",
    "                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n",
    "                            count=int(width)*int(height),\n",
    "                            offset=len(header)\n",
    "                            ).reshape((int(height), int(width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PpmImagePlugin.PpmImageFile image mode=L size=92x112 at 0x7F801D22BFD0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(\"att-database-of-faces/s1/3.pgm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = read_image('att-database-of-faces/s1/1.pgm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 92)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2\n",
    "total_sample_size = 10000\n",
    "n_of_persons = 35\n",
    "\n",
    "def get_data(size, total_sample_size):\n",
    "    #read the image\n",
    "    image = read_image('att-database-of-faces/s' + str(1) + '/' + str(1) + '.pgm', 'rw+')\n",
    "    #reduce the size\n",
    "    #image = image[::size, ::size]\n",
    "    #get the new size\n",
    "    dim1 = image.shape[0]\n",
    "    dim2 = image.shape[1]\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n",
    "    x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])  # 2 is for pairs\n",
    "    y_genuine = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    for i in range(n_of_persons):\n",
    "        for j in range(int(total_sample_size/n_of_persons)):\n",
    "            ind1 = 0\n",
    "            ind2 = 0\n",
    "            \n",
    "            #read images from same directory (genuine pair)\n",
    "            while ind1 == ind2:\n",
    "                ind1 = np.random.randint(10)\n",
    "                ind2 = np.random.randint(10)\n",
    "            \n",
    "            # read the two images\n",
    "            img1 = read_image('att-database-of-faces/s' + str(i+1) + '/' + str(ind1 + 1) + '.pgm', 'rw+')\n",
    "            img2 = read_image('att-database-of-faces/s' + str(i+1) + '/' + str(ind2 + 1) + '.pgm', 'rw+')\n",
    "            \n",
    "            #reduce the size\n",
    "#             img1 = img1[::size, ::size]\n",
    "#             img2 = img2[::size, ::size]\n",
    "            \n",
    "            #store the images to the initialized numpy array\n",
    "            x_geuine_pair[count, 0, 0, :, :] = img1\n",
    "            x_geuine_pair[count, 1, 0, :, :] = img2\n",
    "            \n",
    "            #as we are drawing images from the same directory we assign label as 1. (genuine pair)\n",
    "            y_genuine[count] = 1\n",
    "            count += 1\n",
    "\n",
    "    count = 0\n",
    "    x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n",
    "    y_imposite = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    for i in range(int(total_sample_size/10)):\n",
    "        for j in range(10):\n",
    "            \n",
    "            #read images from different directory (imposite pair)\n",
    "            while True:\n",
    "                ind1 = np.random.randint(n_of_persons)\n",
    "                ind2 = np.random.randint(n_of_persons)\n",
    "                if ind1 != ind2:\n",
    "                    break\n",
    "                    \n",
    "            img1 = read_image('att-database-of-faces/s' + str(ind1+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
    "            img2 = read_image('att-database-of-faces/s' + str(ind2+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
    "\n",
    "#             img1 = img1[::size, ::size]\n",
    "#             img2 = img2[::size, ::size]\n",
    "\n",
    "            x_imposite_pair[count, 0, 0, :, :] = img1\n",
    "            x_imposite_pair[count, 1, 0, :, :] = img2\n",
    "            #as we are drawing images from the different directory we assign label as 0. (imposite pair)\n",
    "            y_imposite[count] = 0\n",
    "            count += 1\n",
    "            \n",
    "    #now, concatenate, genuine pairs and imposite pair to get the whole data\n",
    "    X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255\n",
    "    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_data(size, total_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2, 1, 112, 92)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split our data for training and testing with 75% training and 25% testing proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_network(input_shape):\n",
    "    \n",
    "    seq = Sequential()\n",
    "    \n",
    "    nb_filter = [6, 12]\n",
    "    kernel_size = 3\n",
    "    \n",
    "    \n",
    "    #convolutional layer 1\n",
    "    seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape,\n",
    "                          border_mode='valid', dim_ordering='th'))\n",
    "    seq.add(Activation('relu'))\n",
    "    seq.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    seq.add(Dropout(.25))\n",
    "    \n",
    "    #convolutional layer 2\n",
    "    seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, border_mode='valid', dim_ordering='th'))\n",
    "    seq.add(Activation('relu'))\n",
    "    seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th')) \n",
    "    seq.add(Dropout(.25))\n",
    "\n",
    "    #flatten \n",
    "    seq.add(Flatten())\n",
    "    seq.add(Dense(128, activation='relu'))\n",
    "    seq.add(Dropout(0.1))\n",
    "    seq.add(Dense(50, activation='relu'))\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[2:]\n",
    "img_a = Input(shape=input_dim)\n",
    "img_b = Input(shape=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_network = build_base_network(input_dim)\n",
    "feat_vecs_a = base_network(img_a)\n",
    "feat_vecs_b = base_network(img_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "rms = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input=[img_a, img_b], output=distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our loss function as contrastive_loss function and compile the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=contrastive_loss, optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = x_train[:, 0]\n",
    "img2 = x_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 11250 samples, validate on 3750 samples\n",
      "Epoch 1/5\n",
      "11250/11250 [==============================] - 171s 15ms/step - loss: 0.2194 - val_loss: 0.1528\n",
      "Epoch 2/5\n",
      "11250/11250 [==============================] - 180s 16ms/step - loss: 0.0871 - val_loss: 0.0812\n",
      "Epoch 3/5\n",
      "11250/11250 [==============================] - 261s 23ms/step - loss: 0.0622 - val_loss: 0.0510\n",
      "Epoch 4/5\n",
      "11250/11250 [==============================] - 171s 15ms/step - loss: 0.0510 - val_loss: 0.0401\n",
      "Epoch 5/5\n",
      "11250/11250 [==============================] - 148s 13ms/step - loss: 0.0432 - val_loss: 0.0430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f7e2c44ae80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([img_1, img2], y_train, validation_split=.25,\n",
    "          batch_size=128, verbose=1, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we make predictions with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([x_test[:, 0], x_test[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    return labels[predictions.ravel() < 0.5].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check our model accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9041044776119403"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('face_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 112, 92)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 112, 92)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 50)           1764158     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,764,158\n",
      "Trainable params: 1,764,158\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
